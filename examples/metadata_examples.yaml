# Example Metadata Configurations for Lakeflow Job Meta Framework
# This file demonstrates different task types and use cases

metadata_version: "1.0"

jobs:
  # Example 1: SQL Query Task with Job-Level and Task-Level Settings
  - job_name: "data_quality_checks"
    description: "Simple data quality validation using inline SQL query (no table dependencies)"
    job_config:
      timeout_seconds: 7200
      max_concurrent_runs: 1
      queue:
        enabled: true
      continuous:
        pause_status: UNPAUSED
        task_retry_mode: ON_FAILURE
    tasks:
      - task_key: "simple_quality_check"
        task_type: "sql_query"
        disabled: true  # Task starts as disabled
        timeout_seconds: 3600  # Task-level timeout
        sql_query: |
          SELECT 
            CURRENT_TIMESTAMP() as check_timestamp,
            'data_quality_check' as check_name,
            CAST(5.0 AS DOUBLE) as threshold_value,
            CASE 
              WHEN CAST(5.0 AS DOUBLE) <= 5.0 THEN 'PASS'
              ELSE 'FAIL'
            END as quality_status,
            'Example validation check' as description

  # Example 2: SQL File Tasks
  - job_name: "sales_pipeline"
    description: "Create sample data, then transform and aggregate using SQL files"
    tasks:
      - task_key: "create_sample_data"
        task_type: "sql_file"
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/01_create_sample_data.sql"
        parameters:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"
      
      - task_key: "transform_to_silver"
        task_type: "sql_file"
        depends_on: ["create_sample_data"]
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/03_bronze_to_silver_transformation.sql"
        parameters:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"
      
      - task_key: "daily_aggregations"
        task_type: "sql_file"
        depends_on: ["transform_to_silver"]
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/02_daily_aggregations.sql"
        parameters:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"

  # Example 3: Mixed Task Types
  - job_name: "ingestion_and_validation"
    description: "Ingest data via notebook, then validate with SQL"
    tasks:
      - task_key: "delta_table_ingestion"
        task_type: "notebook"
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/notebook_task/sample_ingestion_notebook"
        parameters:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"
          source_table: "source_customers"
          target_table: "customers"
          write_mode: "append"
      
      - task_key: "validate_data"
        task_type: "sql_query"
        depends_on: ["delta_table_ingestion"]
        sql_query: |
          SELECT 
            CURRENT_TIMESTAMP() as validation_timestamp,
            'data_validation' as validation_type,
            'SUCCESS' as status,
            'Ingestion completed successfully' as message

  # Example 4: SQL Tasks with Saved Queries
  - job_name: "reporting_queries"
    description: "Use pre-saved SQL queries from Databricks SQL"
    tasks:
      - task_key: "daily_sales_report"
        task_type: "sql_query"
        query_id: "911c517b-d93e-4ab1-b065-ce361df23f8b"
        parameters:
          threshold: "5.0"

  # Example 5: SQL File Task - Data Freshness Monitoring
  - job_name: "data_freshness_monitoring"
    description: "Monitor data freshness across multiple tables using SQL file task"
    tasks:
      - task_key: "check_data_freshness"
        task_type: "sql_file"
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/04_data_freshness_check.sql"
        parameters:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"
          max_hours: "24"
