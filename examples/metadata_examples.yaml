# Example Metadata Configurations for Lakeflow Job Meta Framework
# This file demonstrates different task types and use cases

metadata_version: "1.0"

modules:
  # Example 1: SQL Query Task
  - module_name: "data_quality_checks"
    description: "Simple data quality validation using inline SQL query (no table dependencies)"
    sources:
      - source_id: "simple_quality_check"
        source_type: "sql"
        execution_order: 1
        transformation_config:
          task_type: "sql_query"
          sql_task:
            sql_query: |
              SELECT 
                CURRENT_TIMESTAMP() as check_timestamp,
                'data_quality_check' as check_name,
                CAST(5.0 AS DOUBLE) as threshold_value,
                CASE 
                  WHEN CAST(5.0 AS DOUBLE) <= 5.0 THEN 'PASS'
                  ELSE 'FAIL'
                END as quality_status,
                'Example validation check' as description

  # Example 2: SQL File Tasks
  - module_name: "sales_pipeline"
    description: "Create sample data, then transform and aggregate using SQL files"
    sources:
      - source_id: "create_sample_data"
        source_type: "sql"
        execution_order: 1
        transformation_config:
          task_type: "sql_file"
          sql_task:
            sql_file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_job_meta/examples/sql_file_task/01_create_sample_data.sql"
            parameters:
              catalog: "fe_ppark_demo"
              schema: "lakeflow_job_metadata"
      
      - source_id: "transform_to_silver"
        source_type: "sql"
        execution_order: 2
        transformation_config:
          task_type: "sql_file"
          sql_task:
            sql_file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_job_meta/examples/sql_file_task/03_bronze_to_silver_transformation.sql"
            parameters:
              catalog: "fe_ppark_demo"
              schema: "lakeflow_job_metadata"
      
      - source_id: "daily_aggregations"
        source_type: "sql"
        execution_order: 3  # Depends on transform_to_silver
        transformation_config:
          task_type: "sql_file"
          sql_task:
            sql_file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_job_meta/examples/sql_file_task/02_daily_aggregations.sql"
            parameters:
              catalog: "fe_ppark_demo"
              schema: "lakeflow_job_metadata"

  # Example 3: Mixed Task Types
  - module_name: "ingestion_and_validation"
    description: "Ingest data via notebook, then validate with SQL"
    sources:
      - source_id: "delta_table_ingestion"
        source_type: "delta_table"
        execution_order: 1
        source_config:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"
          table: "source_customers"
        target_config:
          catalog: "fe_ppark_demo"
          schema: "lakeflow_job_metadata"
          table: "customers"
          write_mode: "append"
        transformation_config:
          task_type: "notebook"
          notebook_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_job_meta/examples/notebook_task/sample_ingestion_notebook"
      
      - source_id: "validate_data"
        source_type: "sql"
        execution_order: 2
        transformation_config:
          task_type: "sql_query"
          sql_task:
            sql_query: |
              SELECT 
                CURRENT_TIMESTAMP() as validation_timestamp,
                'data_validation' as validation_type,
                'SUCCESS' as status,
                'Ingestion completed successfully' as message

  # Example 4: SQL Tasks with Saved Queries
  - module_name: "reporting_queries"
    description: "Use pre-saved SQL queries from Databricks SQL"
    sources:
      - source_id: "daily_sales_report"
        source_type: "sql"
        execution_order: 1
        transformation_config:
          task_type: "sql_query"
          sql_task:
            query_id: "911c517b-d93e-4ab1-b065-ce361df23f8b"
            parameters:
              threshold: "5.0"

  # Example 5: SQL File Task - Data Freshness Monitoring
  - module_name: "data_freshness_monitoring"
    description: "Monitor data freshness across multiple tables using SQL file task"
    sources:
      - source_id: "check_data_freshness"
        source_type: "sql"
        execution_order: 1
        transformation_config:
          task_type: "sql_file"
          sql_task:
            sql_file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_job_meta/examples/sql_file_task/04_data_freshness_check.sql"
            parameters:
              catalog: "fe_ppark_demo"
              schema: "lakeflow_job_metadata"
              max_hours: "24"

